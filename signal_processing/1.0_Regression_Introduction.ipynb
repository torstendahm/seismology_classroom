{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7306f929",
   "metadata": {},
   "source": [
    "# Regression Analysis and Curve-Fitting\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will learn how to apply a regression on simplified, synthetic data. \n",
    "The regression is used to find a relationship between one (or multiple) independend variable(s) and one dependend variable. We focus here one single dependend variable. The result is a regression curve/model. Afterwards, this model can be used to predict new continuous numerical data. In the last part, we will talk about the basic concepts of 'models', as the (linear) regression is a relative simple model. [More to Regression.](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "\n",
    "\n",
    "<div class =\"alert alert-info\">\n",
    "    The term \"regression\" is used when predicting a numerical, continous variable, while \"classification\" is the term when predicting a discrete variable. Although the *Logistic Regression* includes the term regression, it is acutally an algorithm used for (binary) classification problems.\n",
    "</div>\n",
    "\n",
    "Is used/needed for the following lectures:\n",
    "- Inversion \n",
    "- ....\n",
    " \n",
    "### Table of Contents\n",
    "- [Linear Regression](#Linear_Regression)\n",
    "- [Polynomial Regression](#Polynomial_Regression)\n",
    "- [What is a Model](#Model)\n",
    "- [Split in Train and Test data](#Split)\n",
    "- [Extrapolation](#Extrapolation)\n",
    "\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3d114",
   "metadata": {},
   "source": [
    "<a id='Linear_Regression'></a> \n",
    "# Linear Regression\n",
    "\n",
    "The linear regression investigates the 'linear' relationship between two variables, whereby one variable is assumed to be dependent on the other.\n",
    "\n",
    "The simplest linear regression equation:\n",
    "\n",
    "$y = x \\cdot m + n$.\n",
    "\n",
    "The goal is to find the parameters $m$ and $n$ that link the input variable $x$ to the output variables $y$ best.\n",
    "\n",
    "To find the optimal pair of $m$ and $n$ for your data, there are different 'techniques' that can be used. The most common one is to minimize an error function, e.g. absolute or least-square-fit, between the forward prediction of the model and the dependent variable of the data. The model with the lowest misfit is defined as best model. [More to linear regression.](https://machinelearningmastery.com/linear-regression-for-machine-learning/)\n",
    "\n",
    "### Least-square\n",
    "The standard procedure is the least-square:\n",
    "\n",
    "Model: $y_i = x_i \\cdot m + n$\n",
    "\n",
    "True data: $ŷ_i $\n",
    "\n",
    "$Error = \\sum_{i=1}^{N} (ŷ_i - y_i)^2 $\n",
    "\n",
    "Find out more about least-squares: [Wikipedia](https://en.wikipedia.org/wiki/Least_squares) or [Here](https://www.mathsisfun.com/data/least-squares-regression.html).\n",
    "\n",
    "In general some assumtions need to be fulfilled: \n",
    "- no error in x, otherwise we need to use orthogonal regression\n",
    "- y-error is gaussian distributed and homoscedastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as num\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a967b",
   "metadata": {},
   "source": [
    "First, we create some synthetic data based on a functional relation between x and y, further we add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98eef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating data\n",
    "# xdata = num.linspace(-100, 100, 100)\n",
    "xdata = num.random.uniform(-100, 100, 100)\n",
    "\n",
    "# Setting random seed to create same noise\n",
    "num.random.seed(0)\n",
    "\n",
    "# Creating gaussian noise \n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# offset variable\n",
    "n = 500\n",
    "\n",
    "# x-y-Function\n",
    "ydata = 5 * xdata + n + noise\n",
    "\n",
    "# Plotting data\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78a73e",
   "metadata": {},
   "source": [
    "For the regression, we will use [numpy.polyfit](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). The function needs our input (x) and output (y) data. The model is fitted and returns several parameters. First, the coefficients, here m and n of the regression function. The second value is the residual showing 'how good the fit is'  (the lower the better). The others are not of importance at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting\n",
    "order = 1 # 1 for linear\n",
    "coeff, residuals, _, _, _ = num.polyfit(xdata, ydata, order, full=True)\n",
    "residuals = residuals[0]\n",
    "print('Coeffs:', coeff)\n",
    "print('Residual:', residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0ac89a",
   "metadata": {},
   "source": [
    "We can use the coefficients now to create a regression curve. Either you manually calculate the predicted values (y) for a given set of input values (x) or you can also use helper-functions, here [numpy.poly1d](https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html). The latter approach can be especailly helpful when the number of coeffiencts become large or their relation is not so simple anymore.\n",
    "\n",
    "\n",
    "\n",
    "Further, we can use those newly calculate 'residuals' values to calculate the so called *coefficient of determination*: R². This value determines how good the regression fits our (new) data in a normalized way, allowing for better interpretation and comparability. A value of 1 means perfect fit, while values of 0, or even lower, correspond to no relation between x and y.\n",
    "\n",
    "Finally we can also have a look at the regression line directly. Visually speaking, a good model is obtained when the data points are close to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "yregr = coeff[0] * xdata + coeff[1]\n",
    "\n",
    "#model = num.poly1d(coeff)\n",
    "#yregr = model(xdata)\n",
    "\n",
    "# R2 score goodness-fit\n",
    "# residuals = sum((ydata - yregr) ** 2)\n",
    "denominator = sum((ydata - num.mean(ydata)) ** 2)\n",
    "r2 = 1 - (residuals / denominator)\n",
    "print('R²: %.3f' % r2)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata, label='Data')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xdata, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xdata, yregr))]))\n",
    "plt.plot(xdata, yregr, color='red', label='Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a562272",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-success\">\n",
    "Tasks\n",
    "    \n",
    "- create different linear functions\n",
    "   \n",
    "- play with different data densities and see how the linear regression behaves\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586eef5",
   "metadata": {},
   "source": [
    "<a id='Polynomial_Regression'></a> \n",
    "# Polynominal Regression\n",
    "\n",
    "Often there is not a 'linear' relation between the two variables, but a polynominal, e.g. a squared or cubic.\n",
    "\n",
    "The former expression is expanded to include higher order terms (here 3rd order):\n",
    "\n",
    "$a * x^3 + b * x^2 + c * x + d + ... = y$\n",
    "\n",
    "All the parameters a, b, c, d, ... will be trained.\n",
    "\n",
    "\n",
    "First we look at an example where the linear regression visiually fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b918e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "# xdata = num.linspace(0, 100, 20)\n",
    "xdata = num.random.uniform(0, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "# offset variable\n",
    "n = 500\n",
    "ydata = xdata**2 + n + noise\n",
    "#ydata = xdata * 1 + n  + noise\n",
    "#ydata = num.sin(xdata * 2 * num.pi * 0.01) + xdata * 0.01 + n\n",
    "\n",
    "\n",
    "# Fitting\n",
    "order = 1\n",
    "coeff, residuals, rank, singular_values, rcond = num.polyfit(xdata, ydata, order, full=True)\n",
    "print('Coeffs:', coeff)\n",
    "print('Residual:', residuals[0])\n",
    "\n",
    "model = num.poly1d(coeff)\n",
    "yregr = model(xdata)\n",
    "\n",
    "# R2 score goodness-fit\n",
    "denominator = sum((ydata - num.mean(ydata)) ** 2)\n",
    "r2 = 1 - (residuals[0] / denominator)\n",
    "print('R²: %.3f' % r2)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata, label='Data')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xdata, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xdata, yregr))]))\n",
    "plt.plot(xdata, yregr, color='red', label='Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf4081",
   "metadata": {},
   "source": [
    "Although the R²-value is high, the visual fit is not satisfying.\n",
    "\n",
    "<div class =\"alert alert-success\">\n",
    "Tasks\n",
    "    \n",
    "- change the order to 2\n",
    "   \n",
    "- play with different functions and orders\n",
    "    \n",
    "- overfitting\n",
    "- underfitting\n",
    "\n",
    "</div>\n",
    "\n",
    "## Additional notes:\n",
    "\n",
    "### Orthogonal Regression\n",
    "Here we assumed that only the y-data has an uncertainty. If both x- and y-data have an uncertainty, one should use the orthogonal regression:\n",
    "\n",
    "Normal Regression             |  Orthogonal Regression\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"pictures/linear_regression.jpeg\" alt=\"https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817\" width=\"500\"/> | <img src=\"pictures/orthogonal_regression.png\" alt=\"https://en.wikipedia.org/wiki/Total_least_squares\" width=\"300\"/>\n",
    "\n",
    "\n",
    "### Multiple linear regression\n",
    "It is also possible to include multiple input variables, $x_1, x_2, ...$:\n",
    "\n",
    "$x_1 * a + x_2 * b + c = y$ \n",
    "\n",
    "\n",
    "<img src=\"pictures/multiple_regression.png\" alt=\"https://www.inwt-statistics.de/blog-artikel-lesen/Multiple_lineare_Regression.html\" width=\"300\"/>\n",
    "\n",
    "### Non-linear regression: \n",
    "Is not soo easy and needs a different approach for 'solving' it, this is a major part of the 'Inversion Module'. [See more](https://en.wikipedia.org/wiki/Nonlinear_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6056fd1",
   "metadata": {},
   "source": [
    "<a id='Model'></a> \n",
    "# Model\n",
    "\n",
    "The (multiple) linear regression is a special case of model(/machine) learning. The general goal is to create a model (whatever function) that can be used either to explain the data or to predict new data. Those models are usually based on maths or physics. \n",
    "\n",
    "One python toolbox that provides a lot of different, useful models (not only regression) is [scikit-learn](https://scikit-learn.org/stable/index.html). A great advantage is the consistent and easy-to-understand handling of the model.\n",
    "\n",
    "An exemplarily selection of models:\n",
    "- Linear Regression\n",
    "- Polynomial\n",
    "- Logistic Regression\n",
    "- Perceptron/Neural Networks\n",
    "- Support-Vector-Machines\n",
    "- Stochastic Gradient Descent\n",
    "- Nearest Neighbor\n",
    "- Decision Tree\n",
    "- Clustering\n",
    "- ...\n",
    "\n",
    "\n",
    "To get to know this package, we start with a linear model. For that we need to load the regression model from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Linear Regression Model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initializing the model/ Create python object\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e68c24",
   "metadata": {},
   "source": [
    "Before fitting the data we need to reshape our input data, as it is requested by the model. Afterwards, we give the model with .fit() our input (x) and output (y) data. The model is fitted and can be used to calculated some quality score and the regression parameters, similar as before. Here, the R²-score is already implemented as a function of our model.\n",
    "\n",
    "Afterwards, newly data can be easily predicted with .predict(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555867d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "# xdata = num.linspace(-100, 100, 100)\n",
    "xdata = num.random.uniform(-100, 100, 100)\n",
    "\n",
    "# Setting random seed to create same noise\n",
    "num.random.seed(0)\n",
    "\n",
    "# Creating gaussian noise \n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# offset variable\n",
    "n = 500\n",
    "\n",
    "# x-y-Function\n",
    "ydata = 5 * xdata + n + noise\n",
    "\n",
    "# Reshaping xdata to fit into model requirements \n",
    "xdata = xdata.reshape((-1, 1))\n",
    "\n",
    "# Fitting the model to the data (or vice-versa?)\n",
    "model.fit(xdata, ydata)\n",
    "\n",
    "# Retrieving the quality score and coeffients\n",
    "r_sq = model.score(xdata, ydata)\n",
    "print('Train-R² :', r_sq)\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope    :', model.coef_)\n",
    "\n",
    "#yregr = model.coef_ * xdata + model.intercept_\n",
    "yregr = model.predict(xdata)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata, label='Data')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xdata, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xdata, yregr))]))\n",
    "plt.plot(xdata, yregr, color='red', label='Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#  gibts noch unsicherheiten die das model rausgibt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c94f7",
   "metadata": {},
   "source": [
    "The nice handling of sklearn models: all models can be fitted with the .fit() and prediction values can be created by using .predict(). Therefore, the model can be easily exchanged, without changing anything of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0c991",
   "metadata": {},
   "source": [
    "Without going into to much detail, in sklearn there is no polynomial model directly, but there is the option to combine two functionalities to recreate it. First a polynomial feature selector followed by the known linear regression model is put into a 'pipeline' or 'chain'. In end it behaves just as the numpy pendant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481df77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Combining a the linear model with a polynomial features selector in sklearn\n",
    "order = 2\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=order)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "\n",
    "# Creating data\n",
    "# xdata = num.linspace(0, 100, 100)\n",
    "xdata = num.random.uniform(-100, 100, 50)\n",
    "\n",
    "# Setting random seed to create same noise\n",
    "num.random.seed(0)\n",
    "\n",
    "# Creating gaussian noise \n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# offset variable\n",
    "n = 500\n",
    "\n",
    "# x-y-Function\n",
    "ydata = xdata ** 2 + n + noise\n",
    "\n",
    "\n",
    "# Reshaping\n",
    "xdata = xdata.reshape((-1, 1))\n",
    "\n",
    "# Fitting\n",
    "model.fit(xdata, ydata)\n",
    "\n",
    "# Predicting\n",
    "yregr = model.predict(xdata)\n",
    "r_sq = model.score(xdata, ydata)\n",
    "print('Train-R² :', r_sq)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata, label='Data')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xdata, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xdata, yregr))]))\n",
    "plt.plot(xdata, yregr, color='red', label='Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1576d14",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-success\">\n",
    "Tasks\n",
    "    \n",
    "- select a new regression model from sklearn, while trying different data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e58f2a",
   "metadata": {},
   "source": [
    "<a id='Split'></a> \n",
    "# Split in train and test datasets\n",
    "\n",
    "Good practise in the machine learning community is to split the data into at least a training and a test set. The idea is that the model is learned with the help of the training data, while the test data is afterwards used to check how good the learned model performs on 'new', unseen data. If the score/fit of the test data is sufficient one speaks that the model generalized the problem sufficiently.\n",
    "\n",
    "There are multiple ways of splitting the data, e.g.:\n",
    "- randomly select data points\n",
    "- with a specific scheme: e.g. time based (e.g. last/first samples) \n",
    "\n",
    "First we test the random selection. There is help from sklearn for creating random subsets of our data. Afterwards, we can calculate the R²-score for the train set, but more importantly is the value for the test set. Usually, the train score is higher than the test score, but for good model both values are high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "# xdata = num.linspace(-100, 100, 10)\n",
    "xdata = num.random.uniform(-100, 100, 10)\n",
    "\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# offset variable\n",
    "n = 500\n",
    "\n",
    "# x-y-Function\n",
    "ydata = xdata * 5 + n + noise\n",
    "\n",
    "# Split data randomly into xx% train and xx% test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.33, random_state=42)\n",
    "\n",
    "# Reshaping\n",
    "xtrain = xtrain.reshape((-1, 1))\n",
    "xtest = xtest.reshape((-1, 1))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fitting\n",
    "model = LinearRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "print('Train-Intercept:', model.intercept_)\n",
    "print('Train-Slope    :', model.coef_)\n",
    "\n",
    "# Predicting train data\n",
    "yregr = model.predict(xtrain)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "print('Train-R²       :', train_rsq)\n",
    "\n",
    "# Predicting test data\n",
    "ypred = model.predict(xtest)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xtrain, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xtrain, yregr))]))\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xtest, ypred = list(zip(*[(x,y) for x, y in sorted(zip(xtest, ypred))]))\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89715c31",
   "metadata": {},
   "source": [
    "<div class =\"alert alert-success\">\n",
    "Tasks\n",
    "    \n",
    "- play around with the data density\n",
    "    \n",
    "- change the train-test split sizes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb261ff",
   "metadata": {},
   "source": [
    "<a id='Extrapolation'></a> \n",
    "# Extrapolation\n",
    "In general, a model is used for predicting values within its training range (within the range of the input data). Therefore, the trained model can be used for interpolation, meaning predicting y-values for x-values that were not part of the initial training set, e.g. between two x-points. There are a lot of interpolation techniques that can be used when no functional form can be determined/is known, e.g. mean of neighbors, cubic splines etc.. \n",
    "\n",
    "On the other hand, extrapolation is the term when predicting values that are outside of the training range. Usually, this is not a good idea as the model wasn't trained for this range. But there are cases where extrapolation is the main goal of the model. For example, predicting future data in weather forecast.\n",
    "\n",
    "We can simulate this by selecting the last part of our data as test data for a simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(-100, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# offset variable\n",
    "n = 500\n",
    "# x-y-Function\n",
    "ydata = xdata * 5 + n + noise\n",
    "\n",
    "\n",
    "## Splitting by indices\n",
    "## Selecting the first data as training and the last as testing\n",
    "index = int(len(xdata)/2)\n",
    "xtrain = xdata[:index]\n",
    "ytrain = ydata[:index]\n",
    "\n",
    "xtest = xdata[index:]\n",
    "ytest = ydata[index:]\n",
    "\n",
    "xtrain = xtrain.reshape((-1, 1))\n",
    "xtest = xtest.reshape((-1, 1))\n",
    "\n",
    "\n",
    "# Fitting and plotting\n",
    "model = LinearRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Predicting\n",
    "yregr = model.predict(xtrain)\n",
    "ypred = model.predict(xtest)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Train-Intercept:', model.intercept_)\n",
    "print('Train-Slope    :', model.coef_)\n",
    "print('Train-R²       :', train_rsq)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xtrain, yregr = list(zip(*[(x,y) for x, y in sorted(zip(xtrain, yregr))]))\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "\n",
    "# sorting the data, according to the xdata-values, to generate a 'clear' line\n",
    "xtest, ypred = list(zip(*[(x,y) for x, y in sorted(zip(xtest, ypred))]))\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction outsie of training')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7780d",
   "metadata": {},
   "source": [
    "As we see now, the learned regression does not fit the test so well.  \n",
    "\n",
    "<div class =\"alert alert-success\">\n",
    "Tasks\n",
    "    \n",
    "- play around with the data density\n",
    "    \n",
    "- change the train-test split sizes\n",
    "    \n",
    "- change the functional form to e.g. quadratic\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16046e8",
   "metadata": {},
   "source": [
    "<a id='Summary'></a> \n",
    "# Summary\n",
    "\n",
    "We have learned\n",
    "- the basic ideas behind regression for linear data and polynominal data\n",
    "- leared models can be used for prediction of new data within the training range (interpolation) or simply representing it\n",
    "- it is better to split the data before training in at least a training and a testing set\n",
    "- extrapolation should be avoid, unless the goal of our model is to predict future or extrem data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
