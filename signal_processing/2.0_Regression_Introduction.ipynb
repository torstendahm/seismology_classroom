{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7306f929",
   "metadata": {},
   "source": [
    "# 1D-Regression Analysis\n",
    "\n",
    "\n",
    "#### Paul Vessel??\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will learn how to apply a regression on simplified, synthetic data. \n",
    "The regression is to learn/find a relationship between one or multiple independend variables and one dependend variable. We focus here one single dependend variable. The result is a regression model. Afterwards, this model can be used to predict new continuous numerical data. [More](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "\n",
    "\n",
    "<div class =\"alert alert-info\">\n",
    "    The term \"regression\" is used when predicting a numerical, continous variable, while \"classification\" is the term when predicting a discrete variable. Although the 'Logistic Regression' includes the term regression, it is acutally an algorithm for classification.\n",
    "</div>\n",
    "\n",
    "Possible applications are:\n",
    "- Gutenberg-Richter-Law\n",
    "- Wadati-Diagram\n",
    "\n",
    "Is used/needed for the following lectures:\n",
    "- Inversion\n",
    "- ....\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "- [Linear Regression](#Linear_Regression)\n",
    "- [Split in Train and Test data](#Split)\n",
    "- [Extrapolation](#Extrapolation)\n",
    "- [Polynomial Regression](#Polynomial_Regression)\n",
    "- [Summary](#Summary)\n",
    "\n",
    "\n",
    "\n",
    "### Learning a model, how?\n",
    "In short:\n",
    "Minimizing an error function, e.g. RMS (root-mean-square) error, or in other words least-square-fit, between the forward prediction of the model and the dependent variable of the data. The model with the lowest misfit is defined as best model. Find out more: [Wiki](https://en.wikipedia.org/wiki/Least_squares) and/or [More](https://www.mathsisfun.com/data/least-squares-regression.html) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57a2fc",
   "metadata": {},
   "source": [
    "<a id='Linear_Regression'></a> \n",
    "# Linear Regression\n",
    "\n",
    "Linear model:\n",
    "\n",
    "x * m + n = y \n",
    "\n",
    "Learning the parameters m and n.\n",
    "\n",
    "We use the [LinearRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) model from scikit-learn which uses ordinary least-squares as error function. This functions assumes that only the y-data has a uncertainty. If both x- and y-data have an uncertainty, one should use the orthogonal regression.\n",
    "\n",
    "[More](https://machinelearningmastery.com/linear-regression-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as num\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a967b",
   "metadata": {},
   "source": [
    "First, we create some synthetic data based on a functional relation between x and y, further we add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98eef2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(-100, 100, 200)\n",
    "\n",
    "# Setting random seed to create same noise\n",
    "num.random.seed(0)\n",
    "\n",
    "# Creating gaussian noise \n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# x-y-Function\n",
    "ydata = 5 * xdata + noise\n",
    "\n",
    "# Plotting data\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e13378",
   "metadata": {},
   "source": [
    "Next, we load the regression model from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Linear Regression Model from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Setting the model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e68c24",
   "metadata": {},
   "source": [
    "Before fitting the data we need to reshape our input data, as it is requested by the model. Afterwards, we give the model our input (x) and output (y) data. The model is fitted and can be used to calculated some quality score and the regression parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555867d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping xdata to fit into model requirements \n",
    "xdata = xdata.reshape((-1, 1))\n",
    "\n",
    "# Fitting the model to the data (or vice-versa?)\n",
    "model.fit(xdata, ydata)\n",
    "\n",
    "# Retrieving the quality score and coeffients\n",
    "r_sq = model.score(xdata, ydata)\n",
    "print('Train-R² :', r_sq)\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Slope    :', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c94f7",
   "metadata": {},
   "source": [
    "The R²-score gives an idea how good the model fits the data. A value of 1 is perfect. \n",
    "The intercept and slope values shoud be close to the values of the function you defined for creating the synthetic data.\n",
    "\n",
    "Finally we can also have a look at the model directly - the regression line. Visually speaking, a good model is obtained when the data points are close to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b466baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a regression line and plotting it\n",
    "## There are two ways to do the prediction. One is calculating it manually with the known relation. \n",
    "## But sklearn also provides a .predict() functionality which is handy, especially for more complex models.\n",
    "yregr = model.coef_ * xdata + model.intercept_\n",
    "#yregr = model.predict(xtrain)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xdata, ydata, label='Data')\n",
    "plt.plot(xdata, yregr, color='red', label='Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e58f2a",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- create different linear functions\n",
    "- play with different data densities and see how the linear regression behaves\n",
    "\n",
    "\n",
    "\n",
    "<a id='Split'></a> \n",
    "# Split in train and test datasets\n",
    "\n",
    "Good practise in the machine learning community is to split the data into at least a training and a test set. The idea is that the model is learned with the help of the training data, while the test data is afterwards used to check how good the learned model performs with 'new', unseen data. If the score/fit of the test data is sufficient one speaks that the model generalized the problem sufficient enough.\n",
    "\n",
    "There are multiple ways of splitting the data:\n",
    "- randomly select data points\n",
    "- with a specific scheme: e.g. time based (e.g. last/first) points or points with a specific charateristic \n",
    "\n",
    "First we test the random selection. There is also a help from sklearn that helps us creating subsets of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(-100, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "\n",
    "# x-y-Function\n",
    "ydata = 5 * xdata + noise\n",
    "\n",
    "# Split data randomly into xx% train and xx% test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.33, random_state=42)\n",
    "\n",
    "# Reshaping\n",
    "xtrain = xtrain.reshape((-1, 1))\n",
    "xtest = xtest.reshape((-1, 1))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fitting\n",
    "model = LinearRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "print('Train-Intercept:', model.intercept_)\n",
    "print('Train-Slope    :', model.coef_)\n",
    "\n",
    "# Predicting train data\n",
    "yregr = model.predict(xtrain)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "print('Train-R²       :', train_rsq)\n",
    "\n",
    "# Predicting test data\n",
    "ypred = model.predict(xtest)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89715c31",
   "metadata": {},
   "source": [
    "We see that the regression lines are close to the train and test data points, further, the R² values are close to 1.\n",
    "\n",
    "### Tasks\n",
    "- play around with the data density\n",
    "- change the train-test split sizes   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb261ff",
   "metadata": {},
   "source": [
    "<a id='Extrapolation'></a> \n",
    "# Extrapolation\n",
    "Extrapolation is the term when predicting values that are outside of the training set range. Usually, this is not a good idea as the model wasn't trained for this range. But there are cases where extrapolation is the main goal of the model. For example, predicting future data in weather forecast.\n",
    "\n",
    "If we now change the split method to first/last, we simulate this extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(-100, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "ydata = 5 * xdata + noise\n",
    "\n",
    "## Splitting by indices\n",
    "## Selecting the first data as training and the last as testing\n",
    "index = int(len(xdata)/2)\n",
    "xtrain = xdata[:index]\n",
    "ytrain = ydata[:index]\n",
    "\n",
    "xtest = xdata[index:]\n",
    "ytest = ydata[index:]\n",
    "\n",
    "xtrain = xtrain.reshape((-1, 1))\n",
    "xtest = xtest.reshape((-1, 1))\n",
    "\n",
    "\n",
    "# Fitting and plotting\n",
    "model = LinearRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Predicting\n",
    "yregr = model.predict(xtrain)\n",
    "ypred = model.predict(xtest)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Train-Intercept:', model.intercept_)\n",
    "print('Train-Slope    :', model.coef_)\n",
    "print('Train-R²       :', train_rsq)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction outside of training')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7780d",
   "metadata": {},
   "source": [
    "What we now see is that the regression line learned on the training data does not fit the test so well as before.  \n",
    "\n",
    "### Tasks\n",
    "- play around with the data density\n",
    "- change the train-test split sizes   \n",
    "\n",
    "\n",
    "<a id='Polynomial_Regression'></a> \n",
    "# Polynominal Regression\n",
    "or Multi Linear Regression.\n",
    "\n",
    "Often there is not a linear relation between the two variables, but a polynominal, e.g. a squared or cubic.\n",
    "\n",
    "For that general case the linear formula:\n",
    "\n",
    "x * m + n = y \n",
    "\n",
    "is expanded to include higher order terms (here 3rd order):\n",
    "\n",
    "a * x^3 + b * x^2 + c * x + d = y\n",
    "\n",
    "All the parameters a, b, c, d, ... will be trained.\n",
    "\n",
    "\n",
    "First we look at an example where the linear regression fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(0, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 200, len(xdata))\n",
    "ydata = xdata**2 + noise\n",
    "\n",
    "## Splitting randomly\n",
    "# xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.33, random_state=42)\n",
    "\n",
    "## Splitting by indices\n",
    "index = int(len(xdata)/2)\n",
    "xtrain = xdata[:index]\n",
    "ytrain = ydata[:index]\n",
    "\n",
    "xtest = xdata[index:]\n",
    "ytest = ydata[index:]\n",
    "\n",
    "# Reshaping\n",
    "xtrain = xtrain.reshape((-1, 1))\n",
    "xtest = xtest.reshape((-1, 1))\n",
    "\n",
    "\n",
    "# Fitting and plotting\n",
    "model = LinearRegression()\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Predicting\n",
    "yregr = model.predict(xtrain)\n",
    "ypred = model.predict(xtest)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Train-Intercept:', model.intercept_)\n",
    "print('Train-Slope    :', model.coef_)\n",
    "print('Train-R²       :', train_rsq)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca31d9",
   "metadata": {},
   "source": [
    "Maybe the training data can be fitted roughly well, but the test data shows a completely different behaviour.\n",
    "\n",
    "To fit this data additional parameters are needed: e.g. Polynomial regression. Without going into to much detail, in sklearn there is no polynomial model directly, but there is the option to combine two functionalities to recreate it. First a polynomial feature selector followed by the known linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Combining a the linear model with a polynomial features selector in sklearn\n",
    "model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "\n",
    "# Fitting\n",
    "model.fit(xtrain, ytrain)\n",
    "\n",
    "# Predicting\n",
    "yregr = model.predict(xtrain)\n",
    "ypred = model.predict(xtest)\n",
    "train_rsq = model.score(xtrain, ytrain)\n",
    "test_rsq = model.score(xtest, ytest)\n",
    "print('Train-R²       :', train_rsq)\n",
    "print('Test-R²        :', test_rsq)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction test')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9f3cb",
   "metadata": {},
   "source": [
    "Now, the model fits really good to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f8a32",
   "metadata": {},
   "source": [
    "### With numpy polynomial\n",
    "\n",
    "There is the option to use numpy for the regression. There are slight changes in syntax, e.g. no reshaping needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b918e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data\n",
    "xdata = num.linspace(0, 100, 20)\n",
    "num.random.seed(0)\n",
    "noise = num.random.normal(0, 50, len(xdata))\n",
    "ydata = xdata**2 + noise\n",
    "#ydata = xdata * 1 + noise\n",
    "#ydata = num.sin(xdata * 2 * num.pi * 0.01) + xdata * 0.01\n",
    "\n",
    "\n",
    "## Splitting randomly\n",
    "# xtrain, xtest, ytrain, ytest = train_test_split(xdata, ydata, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "## Splitting by indices\n",
    "index = int(len(xdata)/2)\n",
    "xtrain = xdata[:index]\n",
    "ytrain = ydata[:index]\n",
    "\n",
    "xtest = xdata[index:]\n",
    "ytest = ydata[index:]\n",
    "\n",
    "\n",
    "# Fitting\n",
    "order = 2\n",
    "coeff, residuals, rank, singular_values, rcond = num.polyfit(xtrain, ytrain, order, full=True)\n",
    "print('Coeffs:', coeff)\n",
    "print(residuals)\n",
    "\n",
    "model = num.poly1d(coeff)\n",
    "yregr = model(xtrain)\n",
    "ypred = model(xtest)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.scatter(xtrain, ytrain, label='Train')\n",
    "plt.scatter(xtest, ytest, label='Test')\n",
    "plt.plot(xtrain, yregr, color='red', label='Regression')\n",
    "plt.plot(xtest, ypred, color='red', linestyle=':', label='Prediction outside of training')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dd0a9",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- different number in datasets\n",
    "- different orders\n",
    "- different functions\n",
    "- overfitting\n",
    "- underfitting\n",
    "- prediction outside of learning space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16046e8",
   "metadata": {},
   "source": [
    "<a id='Summary'></a> \n",
    "# Summary\n",
    "\n",
    "We have learned\n",
    "- the basic ideas behind regression for linear data and polynominal data\n",
    "- regression investigates the relationship between (at least) two variables\n",
    "- leared models can be used for prediction of new data or simply representing\n",
    "- that it is better to split the data before training in at least a training and a testing set\n",
    "- extrapolation should be avoid, unless the goal of our model is to predict future or extrem data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
